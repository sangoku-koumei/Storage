import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import streamlit as st
import random

# User-Agent list to rotate
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0"
]

def scrape_coconala_category(category_url, max_pages=1):
    """
    Scrapes Coconala category page for items.
    Returns a DataFrame of items.
    Note: Coconala often blocks scraping. This is a basic implementation.
    """
    items = []
    
    for page in range(1, max_pages + 1):
        url = f"{category_url}?page={page}&ref=header_search" # logic might differ
        headers = {'User-Agent': random.choice(USER_AGENTS)}
        
        try:
            # st.write(f"Accessing: {url}")
            response = requests.get(url, headers=headers)
            if response.status_code != 200:
                st.warning(f"Failed to retrieve page {page}. Status: {response.status_code}")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # This selector is hypothetical and needs adjustment based on actual Coconala structure
            # As of 2024/2026, class names are often obfuscated (e.g., c-searchItem)
            service_cards = soup.find_all('div', class_=lambda x: x and 'c-searchItemClass' in x) 
            
            # Fallback for generic structure search if class names change
            if not service_cards:
                 service_cards = soup.select('a[class*="c-searchItem"]')

            for card in service_cards:
                try:
                    title_tag = card.find('div', class_=lambda x: x and 'title' in x.lower())
                    price_tag = card.find('div', class_=lambda x: x and 'price' in x.lower())
                    rating_counts = card.find('div', class_=lambda x: x and 'count' in x.lower())
                    
                    title = title_tag.get_text(strip=True) if title_tag else "Unknown"
                    price = price_tag.get_text(strip=True) if price_tag else "0"
                    
                    # Basic extraction
                    link = card.get('href')
                    if link and not link.startswith('http'):
                        link = 'https://coconala.com' + link
                        
                    items.append({
                        'title': title,
                        'price': price,
                        'link': link,
                        'is_new': 'æ–°ç€' in card.text or 'NEW' in card.text
                    })
                except Exception:
                    continue
            
            time.sleep(1) # Be polite
            
        except Exception as e:
            st.error(f"Error scraping Coconala: {e}")
            
    # Mock data for demonstration if scraping fails (Anti-Scraping protection is strong)
    if not items:
        st.warning("âš ï¸ ã‚³ã‚³ãƒŠãƒ©ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾ç­–ã«ã‚ˆã‚Šãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
        items = [
            {'title': 'éœŠè¦–ã§å½¼ã®æ°—æŒã¡ã‚’æ·±ãèª­ã¿è§£ãã¾ã™', 'price': '3,000å††', 'link': '#', 'solds': 5, 'is_new': True},
            {'title': 'ã€ç·Šæ€¥ã€‘ä»Šã™ãé€£çµ¡ãŒæ¬²ã—ã„ã‚ãªãŸã¸æ€å¿µä¼é”', 'price': '10,000å††', 'link': '#', 'solds': 12, 'is_new': True},
            {'title': 'ä¸å€«ãƒ»è¤‡é›‘æ„›...æ³¥æ²¼ã‹ã‚‰æ•‘ã„å‡ºã—ã¾ã™', 'price': '15,000å††', 'link': '#', 'solds': 4, 'is_new': True},
            {'title': 'â€»æ‚ªç”¨å³ç¦â€» å½¼ã‚’æ²¼ã‚‰ã›ã‚‹ç¦æ–­ã®LINEè¡“', 'price': '5,000å††', 'link': '#', 'solds': 30, 'is_new': False},
        ]

    return pd.DataFrame(items)

def analyze_strategy(df):
    """
    Analyzes the dataframe to find 'Winning Patterns'.
    """
    if df.empty:
        return "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"

    # Simple Keyword Analysis
    all_text = " ".join(df['title'].tolist())
    
    # Mock analysis since we don't have full NLP here yet
    report = """
    ### ğŸ“Š ã‚³ã‚³ãƒŠãƒ©ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
    
    **1. å£²ã‚Œã¦ã„ã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã®å‚¾å‘**
    - **ã€Œå…·ä½“çš„ã€**: ã€Œå½¼ã€ã§ã¯ãªãã€ŒéŸ³ä¿¡ä¸é€šã®å½¼ã€
    - **ã€Œç·Šæ€¥æ€§ã€**: ã€Œä»Šã™ãã€ã€Œç·Šæ€¥ã€
    - **ã€Œç¦æ­¢ã€**: ã€Œæ‚ªç”¨å³ç¦ã€ã€Œç¦æ–­ã€
    
    **2. æ¨å®šåç›Šæ§‹é€  (æ¾ç«¹æ¢…)**
    - ãƒ•ãƒ­ãƒ³ãƒˆ: 3,000å††ã€œ5,000å†† (é‘‘å®š)
    - ãƒŸãƒ‰ãƒ«: 10,000å†† (ç¸çµã³ãƒ»æ–½è¡“)
    - **å‹ã¡ç­‹**: ã€Œæ–°ç€ã€ã§ãƒ©ãƒ³ã‚¯ã‚¤ãƒ³ã—ã¦ã„ã‚‹å‡ºå“è€…ã¯ã€æ—¢å­˜é¡§å®¢ã‚’LINEã‹ã‚‰èª˜å°ã—ã¦åˆé€Ÿã‚’ã¤ã‘ã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã€‚
    """
    return report
