# 「ことせかい」代替アプリとデータ移行 チャットログ

## Geminiとの会話

**Gemini**

「ことせかい」代替アプリとデータ移行

チャットを新規作成
作成したもの
 の画像
 の画像
 の画像
Gem
チャット
カレンダー自動生成ツールの提供
「ことせかい」代替アプリとデータ移行
インスタのリールで占い師アカウントの投稿を作りたい タロット3枚のバズる画像を作成して
鑑定書の表紙を作って 文字は自分で入れるので文字はなしでスペースだけあけて 縦長サイズでしっかりとした鑑定書っぽく 画像を生成して
Gemsの販売・共有・無料利用
ネットビジネスで月10〜20万稼ぐ方法
PythonツールをWebで動かす環境
インスタ占い師 LINE連携戦略
Note記事販売のコツと設定エラー
Note記事販売のコツと設定エラー
Gmail設定エラーでNote記事執筆不可
ココナラ占い販売の成功戦略
最初にみつけた３つの言葉　という心理テスト問題を作成するGPTｓのアイコン画像を作成して 問題例 おもいやりたくのそ ねはふみこさつひじ ちまるとけうしえぶ ほゃなすらぎたけん そのれふみやこさじ つひまんるすとけく うしえぶじらほなす たくぎのそぎねはふ みこさつひじたくの
アイコン画像を生成したい 「ミニマルなロゴ。角丸のタロットカードのシルエット1枚。中央に三日月と八芒星のシンプルな切り抜き。太めの線、フラットな塗り。配色は背景が濃紺、図形は金、差し色に青緑を少量。文字なし、枠なし、影なし、細かい装飾なし。高コントラストで小さくても判読できる」
画像生成 縦長 Cute Japanese manga style vertical illustration,   watercolor hand drawn rough sketch,   soft romantic lighting,   subtle pastel background with one vivid accent color,   strong central focal point on the main character or symbol,   scene inspired by the meaning of Page of Cups in love reading,   emotion just after pretending to smile despite emotional pain,   a young woman sitting quietly with a gentle smile, holding a small cup near her chest,   wearing soft casual clothes, delicate expression,   surrounded by warm but slightly blurred colors, hint of loneliness in the air,   no frame, no border, no tarot card, no card layout,   no text, no letters, no words, no numbers, no symbols, no subtitles.
不安の暴走列車: あなたの頭の中に、小さな機関車が走っています。機関車の名前は「彼がそっけない」。それが「気持ちが冷めた」駅を通り、「振られる」駅を猛スピードで通過して、「やっぱり私は一人ぼっち」という終着駅に向かって暴走している。そんなシンボリックなイラストです。 文字を日本語にして画像を作り直して
人の手（白い手袋）が　カードAを指さす画像にして
占い師バイブル：稼ぐプロへ
Discordトークン取得方法と注意点
仕事運カレンダーのPDF化とデザイン
Canvas Copied Confirmation
Supabase 個人プロジェクトへの切り替え
ChatGPT エラー111 解決策
不倫関係の苦悩と未来

（中略：プロフィールのやりとり）

**User**
小説の自動音声化は可能？費用は？

音楽アプリに小説入れると車などて音声流したい時に勝手にランダム再生で小説流れたり　今もことせかいで1000冊くらいダウンロードしてるんだけど　そう言う使い方は出来ないよね

**Model**
ご懸念の点、非常によく分かります。「ドライブ中にJ-POPの合間に突然異世界転生の朗読が流れる」のは地獄ですよね。また、1000冊というライブラリの規模を考えると、単にファイルを放り込むだけでは管理不能になります。

ご質問への回答と、**「音楽と混ぜずに、無料で、大量の小説を管理して聴く」**ための現実的な構成を提案します。

1. 自動音声化は可能？費用は？
結論：可能です。そして費用は「0円」で済みます。

技術: Pythonのedge-ttsというライブラリを使います（Microsoft Edgeの読み上げ機能をプログラムから使うものです）。

品質: 非常に高く、イントネーションも自然です。「ことせかい」の標準音声と同等か、それ以上です。

費用: 完全無料です。

※OpenAIなどの有料APIを使うと高額になりますが、個人で聴く分には無料のedge-ttsで十分すぎる品質です。

2. 「音楽と混ざる問題」と「大量データ管理」の解決策
標準の「ミュージック」アプリに入れるのはやめましょう。代わりに、**「オーディオブック専用アプリ」**を使います。

私が最もおすすめするのは、**『BookPlayer』**というiPhoneアプリです。

なぜ『BookPlayer』なのか？
音楽アプリと完全に分離: Apple Music等のライブラリとは混ざりません。車で勝手に流れることはありません。

完全無料 & 広告なし: オープンソースで作られており、非常に使い勝手が良いです。

フォルダ管理: 1000冊あっても、フォルダごとに整理して入れれば崩壊しません。

続きから再生: 小説ごとに「どこまで聴いたか」を記憶してくれます。バックグラウンド再生も完璧です。

推奨する「最強の自炊環境」の作り方
ユーザー様はPythonが扱えるので、以下のフローを構築するのがベストです。

手順①：PCで「MP3生成ツール」を作る（Python）
PC上で動くPythonスクリプトを作ります。

機能: 「小説家になろう」のURL（目次ページ）を渡すと、全話をスクレイピングし、1話ごとにMP3ファイルに変換する。

保存先: PC上の「iCloud Drive」または「Google Drive」フォルダに直接保存させる。

手順②：iPhoneの『BookPlayer』で読み込む
iPhoneでBookPlayerを開く。

「インポート」から、iCloud Drive（またはGoogle Drive）に入ったフォルダを選択。

これで、「ことせかい」のように本棚形式で並び、バックグラウンド再生が可能になります。

⚠️ 注意点：1000冊のデータ容量について
ここが最大のネックです。テキストデータと違い、音声データは容量が巨大です。

計算: 小説1話（約5分）をMP3にすると、音質を少し落としても約3MB〜5MBになります。

長編小説1冊（100話）: 約300MB〜500MB。

1000冊: もし全てが長編なら、300GB〜500GBになります。スマホの容量がパンクします。

現実的な運用: 「1000冊すべてを常に持ち歩く」のは音声ファイルでは不可能です。 「今読んでいる（聴いている）数冊〜数十冊だけを変換してクラウドに入れる」 という運用に切り替える必要があります。

結論とネクストアクション
「ことせかい」の代わりとして、Pythonスクリプト ＋ BookPlayerアプリ の組み合わせが最も有力な解決策です。

もしよろしければ、**「なろうの目次URLを入れると、全話を音声化してフォルダに保存するPythonコード」**を私が書きます。

これを試してみたいですか？ （※Windowsでも動作し、ルビ（ふりがな）をある程度考慮して読み上げるような処理を含めることができます）

**User**
タイマー機能はある？

30分ごとに止まるとか

**Model**
ご安心ください。BookPlayerには優秀なタイマー機能が付いていますし、もっと便利なiPhone本体の裏技もあります。

ですので、Python側でファイルを30分ごとに分割するような面倒な作業は一切不要です。

方法1：BookPlayerの機能を使う（おすすめ）
アプリ自体に専用のスリープタイマーボタン（月のアイコン🌙や「Zzz」マーク）があります。

時間指定: 「15分」「30分」「45分」「60分」など選べます。

「章の終わり」で停止: これが小説には最高です。「キリが良いところ（次のファイルに移る直前）」で止まってくれるので、話の途中でブツッと切れるストレスがありません。

方法2：iPhone標準の「時計」アプリを使う（最強の裏技）
もしBookPlayer以外のアプリを使うことになっても、この方法ならYouTubeでも音楽でも何でも強制的に止められます。

iPhoneの**「時計」**アプリを開く。

右下の**「タイマー」**タブを選ぶ。

**「タイマー終了時」**という部分をタップ。

一番下までスクロールして**「再生停止」**を選んで設定。

時間を「30分」にセットして開始。

これで、30分経つと、どんなアプリで再生していてもフッと音が消えて、ロック画面に戻ります。寝落ち対策にはこれが最強です。

結論
Pythonでやるべきなのは**「全話を章ごとに1つのMP3にする（1話＝1ファイル）」**ことだけです。 あとはアプリ側の機能で「30分で止める」とか「この話が終わったら寝る」といった調整が快適に行なえます。

この構成で、Pythonコードの作成準備に入ってもよろしいでしょうか？

**User**
ok

**Model**
お待たせしました。ご自身の環境（Windows + Python）で動作する、「小説家になろう」専用の音声化ツールを作成しました。

このスクリプトは以下の特徴があります：

ルビ（ふりがな）対応: なろう特有の読み仮名（例: 本気と書いてマジと読むなど）を優先して読み上げるように処理します。

フォルダ整理: 小説のタイトルでフォルダを作り、各話を「001_第1話...」のように番号順に保存します。これで『BookPlayer』に入れた時に順番通り再生されます。

サーバーへの配慮: なろうのサーバーに負荷をかけないよう、1話ごとに少し待機時間を設けています。

1. 準備（ライブラリのインストール）
コマンドプロンプト（またはターミナル）で以下のコマンドを実行して、必要なライブラリを入れてください。

```bash
pip install requests beautifulsoup4 edge-tts asyncio
```

2. Pythonコード
以下のコードを `narou2mp3.py` という名前などで保存してください。

```python
import os
import time
import re
import asyncio
import requests
from bs4 import BeautifulSoup
import edge-tts

# ==========================================
# 設定エリア
# ==========================================
# 変換したい小説の目次ページのURLをここに入れてください
TARGET_URL = "https://ncode.syosetu.com/xxxxxxx/"  

# 音声の種類 (女性: ja-JP-NanamiNeural, 男性: ja-JP-KeitaNeural)
VOICE = "ja-JP-NanamiNeural"

# 再生速度 (例: +0% は標準, +20% は1.2倍速, -10% は0.9倍速)
RATE = "+0%" 

# 音量
VOLUME = "+0%"
# ==========================================

def clean_text(soup):
    """
    小説本文からテキストを抽出する。
    ルビ（ふりがな）がある場合、漢字ではなくルビを優先して採用する処理を行う。
    """
    # 本文を取得（IDはなろうの仕様に基づく）
    content = soup.select_one("#novel_honbun")
    if not content:
        return ""

    # ルビ処理: <ruby>親友<rt>とも</rt></ruby> → "とも" に置換
    # これをしないと「親友とも」と二重に読まれてしまうため
    for ruby in content.find_all("ruby"):
        # rtタグ（ルビ部分）の中身を取得
        rt = ruby.find("rt")
        if rt:
            # ルビのテキストを取得
            ruby_text = rt.get_text()
            # rubyタグ全体をルビのテキストで置き換える
            ruby.replace_with(ruby_text)
    
    # テキストのみ抽出して、不要な空行を削除
    text = content.get_text()
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    return "\n".join(lines)

async def text_to_mp3(text, output_path):
    """テキストを音声ファイルに変換して保存"""
    communicate = edge_tts.Communicate(text, VOICE, rate=RATE, volume=VOLUME)
    await communicate.save(output_path)

def get_novel_info(url):
    """目次ページからタイトルと各話のリンクを取得"""
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    response = requests.get(url, headers=headers)
    response.encoding = response.apparent_encoding
    soup = BeautifulSoup(response.text, "html.parser")

    # タイトル取得
    title_elem = soup.select_one(".novel_title")
    if not title_elem:
        print("小説のタイトルが見つかりません。URLを確認してください。")
        return None, []
    
    title = title_elem.get_text(strip=True)
    
    # Windowsのファイル名に使えない文字を置換
    title = re.sub(r'[\\/:*?"<>|]', '_', title)

    # 各話のリンク取得
    chapters = []
    # リンク要素を取得
    for a in soup.select(".subtitle a"):
        chap_title = a.get_text(strip=True)
        chap_url = "https://ncode.syosetu.com" + a.get("href")
        chapters.append((chap_title, chap_url))
        
    return title, chapters

async def main():
    print("小説情報を取得中...")
    title, chapters = get_novel_info(TARGET_URL)
    
    if not title:
        return

    print(f"小説タイトル: {title}")
    print(f"全 {len(chapters)} 話")

    # 保存用フォルダ作成
    if not os.path.exists(title):
        os.makedirs(title)
        print(f"フォルダ作成: {title}")

    # 各話を処理
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    
    for i, (chap_title, chap_url) in enumerate(chapters, 1):
        # ファイル名: 001_プロローグ.mp3 のように連番を振る
        safe_chap_title = re.sub(r'[\\/:*?"<>|]', '_', chap_title)
        filename = f"{i:03}_{safe_chap_title}.mp3"
        filepath = os.path.join(title, filename)

        # 既にファイルがある場合はスキップ（中断後の再開用）
        if os.path.exists(filepath):
            print(f"スキップ（作成済）: {filename}")
            continue

        print(f"処理中 ({i}/{len(chapters)}): {chap_title}")

        # 本文取得
        try:
            res = requests.get(chap_url, headers=headers)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, "html.parser")
            
            # 本文抽出とルビ処理
            text = clean_text(soup)
            
            # 読み上げるテキスト（タイトル + 本文）
            full_text = f"{chap_title}。\n\n{text}"

            # 音声化
            await text_to_mp3(full_text, filepath)

        except Exception as e:
            print(f"エラー発生: {e}")
            continue

        # サーバー負荷軽減のため待機（重要）
        time.sleep(2)

    print("\nすべての変換が完了しました！")

if __name__ == "__main__":
    asyncio.run(main())
```

3. 使い方
URLの設定: コード内の `TARGET_URL = "..."` の部分を、読みたい小説の目次ページのURLに書き換えてください。

実行: コマンドプロンプトで `python narou2mp3.py` を実行します。

完了後: 実行した場所（フォルダ）に、小説タイトルのフォルダができ、その中にMP3ファイルがずらっと生成されます。

4. iPhoneへの入れ方（ここが重要）
PCで作ったファイルをiPhoneの『BookPlayer』に入れる最もスムーズな手順です。

PC側:

生成された「小説タイトルのフォルダ」ごと、iCloud Drive（またはGoogle Drive/Dropbox）にコピーします。

※Windows用iCloudアプリを入れているなら、エクスプローラーにドラッグ＆ドロップするだけです。

iPhone側:

『BookPlayer』アプリを開きます。

「読み込み（インポート）」を選び、「ファイル」を選択します。

iCloud Drive内の先ほどのフォルダを選択します。

これで、1000冊分の管理も「フォルダごと」に綺麗に行えますし、再生も30分タイマーで快適になるはずです。試してみてください。

**User**
Pythonコードに改善点などはない？

**Model**
提示したコードは「動く」レベルのものでしたが、1000冊規模の小説を扱うとなると、いくつか実用面で足りない部分や、もっと快適にするための改善点があります。

特に**「エラーで止まらないこと」と「物語の余韻（前書き・後書き）」**への対応が重要です。

以下に、**「Ver.2 実用強化版」**として改善コードを作成しました。

主な改善ポイント
前書き・後書きへの対応

以前のコードは「本文」しか取っていませんでした。なろう小説には重要な伏線や作者コメントが「前書き」「後書き」にあることが多いので、これらも取得して結合するようにしました。

エラー時の「自動リトライ」機能

数百話を連続処理していると、一瞬の通信エラーでスクリプトが停止することがあります。止まらずに3回まで再挑戦する機能を追加しました。

進捗バーの表示 (tqdmライブラリ)

「いま何話目？あとどれくらい？」が視覚的にわかるようにしました。

読み上げの「間」を調整

タイトルや章の終わりの後に少し無音（改行）を挟むことで、聞きやすくしました。

1. 追加ライブラリのインストール
進捗バーを表示するために `tqdm` を追加します。

```bash
pip install tqdm
```

2. 改善版 Pythonコード (`narou2mp3_v2.py`)
```python
import os
import time
import re
import asyncio
import requests
from bs4 import BeautifulSoup
import edge-tts
from tqdm import tqdm  # 進捗バー表示用

# ==========================================
# 設定エリア
# ==========================================
TARGET_URL = "https://ncode.syosetu.com/xxxxxxx/"  # ここを書き換える
VOICE = "ja-JP-NanamiNeural"  # 女性: ja-JP-NanamiNeural, 男性: ja-JP-KeitaNeural
RATE = "+0%" 
VOLUME = "+0%"
RETRY_COUNT = 3  # 通信エラー時の再試行回数
# ==========================================

def clean_text(element):
    """
    HTML要素からテキストを抽出し、ルビを読み仮名に変換する
    """
    if not element:
        return ""

    # ルビ処理: <ruby>本気<rt>マジ</rt></ruby> → "マジ"
    # ※コピーを操作して元のsoupを破壊しないようにする
    elem_copy = BeautifulSoup(str(element), "html.parser")
    
    for ruby in elem_copy.find_all("ruby"):
        rt = ruby.find("rt")
        if rt:
            ruby_text = rt.get_text()
            ruby.replace_with(ruby_text)
    
    text = elem_copy.get_text()
    # 空行を整理
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    return "\n".join(lines)

async def text_to_mp3(text, output_path):
    """テキストを音声ファイルに変換"""
    communicate = edge_tts.Communicate(text, VOICE, rate=RATE, volume=VOLUME)
    await communicate.save(output_path)

def get_soup_with_retry(url):
    """通信エラー時にリトライする関数"""
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    for attempt in range(RETRY_COUNT):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            return BeautifulSoup(response.text, "html.parser")
        except Exception as e:
            if attempt < RETRY_COUNT - 1:
                time.sleep(3)  # 3秒待って再試行
            else:
                raise e  # 諦めてエラーを吐く
    return None

def get_novel_info(url):
    """目次ページからタイトルと各話リンクを取得"""
    soup = get_soup_with_retry(url)
    
    title_elem = soup.select_one(".novel_title")
    if not title_elem:
        print("小説タイトルが見つかりません。")
        return None, []
    
    title = title_elem.get_text(strip=True)
    title = re.sub(r'[\\/:*?"<>|]', '_', title) # ファイル名禁止文字除去

    chapters = []
    for a in soup.select(".subtitle a"):
        chap_title = a.get_text(strip=True)
        chap_url = "https://ncode.syosetu.com" + a.get("href")
        chapters.append((chap_title, chap_url))
        
    return title, chapters

async def main():
    print("小説情報を取得中...")
    try:
        title, chapters = get_novel_info(TARGET_URL)
    except Exception as e:
        print(f"目次情報の取得に失敗しました: {e}")
        return

    if not title:
        return

    print(f"タイトル: {title}")
    print(f"全 {len(chapters)} 話")

    # フォルダ作成
    if not os.path.exists(title):
        os.makedirs(title)

    # tqdmを使って進捗バーを表示
    for i, (chap_title, chap_url) in enumerate(tqdm(chapters, desc="変換中"), 1):
        
        safe_chap_title = re.sub(r'[\\/:*?"<>|]', '_', chap_title)
        filename = f"{i:03}_{safe_chap_title}.mp3"
        filepath = os.path.join(title, filename)

        # 既にファイルがある場合はスキップ
        if os.path.exists(filepath):
            continue

        try:
            soup = get_soup_with_retry(chap_url)
            
            # 各パーツの取得
            preface = clean_text(soup.select_one("#novel_p"))  # 前書き
            body = clean_text(soup.select_one("#novel_honbun")) # 本文
            afterword = clean_text(soup.select_one("#novel_a")) # 後書き

            # 結合（間に「間」を入れる）
            # 読点「。」や改行を入れることで、AI音声が少しポーズを取ります
            full_text_parts = []
            
            full_text_parts.append(chap_title + "。\n") # タイトル読み上げ

            if preface:
                full_text_parts.append("前書き。\n" + preface + "\n")
            
            full_text_parts.append(body + "\n")
            
            if afterword:
                full_text_parts.append("後書き。\n" + afterword)

            full_text = "\n".join(full_text_parts)

            # 音声化
            await text_to_mp3(full_text, filepath)

        except Exception as e:
            print(f"\nエラー ({chap_title}): {e}")
            # エラーが出ても次の話へ進む（止まらない）
            continue

        # サーバー負荷軽減
        time.sleep(2)

    print(f"\n完了！ '{title}' フォルダを確認してください。")

if __name__ == "__main__":
    asyncio.run(main())
```

さらに便利にするためのヒント
もし「毎回コード内のURLを書き換えるのが面倒」だと感じたら、最後の部分（`if __name__ == "__main__":` の下）を以下のように変えると、実行時にURLを聞いてくれるようになります。

```python
if __name__ == "__main__":
    # URLを入力させる
    input_url = input("変換したい小説のURLを入力してください: ").strip()
    if input_url:
        TARGET_URL = input_url
        asyncio.run(main())
```
これで、Pythonファイルをダブルクリック（または実行）するだけで、コンソール画面でURLをペーストしてEnterを押すだけで変換が始まります。快適な読書（聴書）ライフを！

**User**
本文だけで良い　前書き　あとがきはいらない

逆に　同じ事を毎話書いている場合や　urlのある文章は読まないなども出来るなら欲しいくらい。

**Model**
承知しました。ユーザー様はStreamlitの経験がおありですので、**「Streamlitを使ったGUIアプリ」**にするのが一番使い勝手が良いです。

ご要望をすべて満たす**「なろう専用オーディオブック生成アプリ」**のコードを作成しました。

このアプリの特徴
GUIで操作: URLを貼り付けてボタンを押すだけ。

本文のみ抽出: 前書き・後書きは完全にカットします。

不要文の削除: URLの自動削除に加え、**「除外したい定型文」**を設定できるようにしました（例: 「ブックマークお願いします」などを登録すれば消えます）。

続きからダウンロード（更新対応）: フォルダ内にすでにファイルがある話数は自動でスキップします。つまり、100話まである状態でボタンを押せば、101話からスタートします。

1話のURLでもOK: 1話のURLを貼っても、自動で「目次ページ」を判定して全話取得するようにしました。

1. 準備
必要なライブラリに `streamlit` を追加します。

```bash
pip install streamlit requests beautifulsoup4 edge-tts
```

2. アプリのコード (`app.py`)
以下のコードを `app.py` という名前で保存してください。

```python
import streamlit as st
import os
import re
import asyncio
import requests
from bs4 import BeautifulSoup
import edge-tts

# ==========================================
# ロジック部分
# ==========================================

def get_soup(url):
    """HTMLを取得してBeautifulSoupオブジェクトを返す"""
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        return BeautifulSoup(response.text, "html.parser")
    except Exception as e:
        st.error(f"URLの取得に失敗しました: {e}")
        return None

def normalize_url_to_index(url):
    """
    1話のURL (.../ncode/xxxxx/1/) が来ても
    目次URL (.../ncode/xxxxx/) に変換する
    """
    # 末尾の数字部分を削除して目次URLにする簡易処理
    # なろうのURLパターン: https://ncode.syosetu.com/nXXXXX/1/ -> nXXXXX/
    match = re.search(r'(https://ncode\.syosetu\.com/n[a-zA-Z0-9]+)', url)
    if match:
        return match.group(1) + "/"
    return url

def clean_text(soup, remove_phrases):
    """
    本文のみ抽出し、URLや指定された定型文を削除する
    """
    # 本文（#novel_honbun）のみ取得
    content = soup.select_one("#novel_honbun")
    if not content:
        return ""

    # ルビを親文字（漢字）ではなく、ふりがな優先に置換
    # コピーを作成して操作
    soup_copy = BeautifulSoup(str(content), "html.parser")
    for ruby in soup_copy.find_all("ruby"):
        rt = ruby.find("rt")
        if rt:
            ruby.replace_with(rt.get_text())

    text = soup_copy.get_text()

    # URLの削除 (http〜 を削除)
    text = re.sub(r'https?://[\w/:%#\$&\?\(\)~\.=\+\-]+', '', text)

    # ユーザー指定の「削除したい定型文」を削除
    if remove_phrases:
        for phrase in remove_phrases.split("\n"):
            phrase = phrase.strip()
            if phrase:
                text = text.replace(phrase, "")

    # 空行の整理
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    return "\n".join(lines)

async def generate_audio(text, filepath, voice, rate):
    """Edge-TTSで音声を生成"""
    communicate = edge_tts.Communicate(text, voice, rate=rate)
    await communicate.save(filepath)

async def process_novel(url, remove_phrases, voice, rate):
    """小説の処理メインループ"""
    
    # URLの正規化（目次ページへ）
    index_url = normalize_url_to_index(url)
    
    soup = get_soup(index_url)
    if not soup:
        return

    # タイトル取得
    title_elem = soup.select_one(".novel_title")
    if not title_elem:
        st.error("小説のタイトルが見つかりませんでした。なろうのURLか確認してください。")
        return

    title = title_elem.get_text(strip=True)
    safe_title = re.sub(r'[\\/:*?"<>|]', '_', title) # フォルダ名用に記号除去
    
    st.success(f"小説タイトル: {title}")
    
    # 保存フォルダ作成
    if not os.path.exists(safe_title):
        os.makedirs(safe_title)

    # 各話リンク取得
    chapters = []
    for a in soup.select(".subtitle a"):
        chap_title = a.get_text(strip=True)
        chap_url = "https://ncode.syosetu.com" + a.get("href")
        chapters.append((chap_title, chap_url))

    if not chapters:
        st.warning("各話のリンクが見つかりません。短編小説か、構造が異なります。")
        return

    # 進捗バーの設定
    progress_bar = st.progress(0)
    status_text = st.empty()
    total_chapters = len(chapters)

    # 処理ループ
    for i, (chap_title, chap_url) in enumerate(chapters, 1):
        safe_chap_title = re.sub(r'[\\/:*?"<>|]', '_', chap_title)
        filename = f"{i:03}_{safe_chap_title}.mp3"
        filepath = os.path.join(safe_title, filename)

        # 【重要】ファイルが存在すればスキップ（続きからダウンロード機能）
        if os.path.exists(filepath):
            status_text.text(f"スキップ（作成済）: {chap_title}")
            progress_bar.progress(i / total_chapters)
            continue

        status_text.text(f"生成中 ({i}/{total_chapters}): {chap_title}")

        # 本文取得
        c_soup = get_soup(chap_url)
        if c_soup:
            # 本文のみ抽出 & 掃除
            body_text = clean_text(c_soup, remove_phrases)
            
            # 読み上げ用テキスト（タイトル + 少し間 + 本文）
            full_text = f"{chap_title}。\n\n{body_text}"
            
            # 音声生成
            try:
                await generate_audio(full_text, filepath, voice, rate)
            except Exception as e:
                st.error(f"音声生成エラー: {chap_title} - {e}")

        # 進捗更新
        progress_bar.progress(i / total_chapters)
        
        # サーバー負荷軽減の待機（Streamlit上では非同期sleepを使う）
        await asyncio.sleep(2)

    status_text.text("すべての処理が完了しました！")
    st.balloons()

# ==========================================
# UI部分 (Streamlit)
# ==========================================

st.set_page_config(page_title="なろう音声化ツール", layout="centered")

st.title("📖 なろう音声化ツール")
st.caption("本文のみ抽出・URL除去・続きからDL対応")

# 入力フォーム
url_input = st.text_input("URLを入力 (目次ページ または 1話目のURL)", placeholder="https://ncode.syosetu.com/xxxxxx/")

# 設定エリア
with st.expander("詳細設定", expanded=True):
    col1, col2 = st.columns(2)
    with col1:
        voice_role = st.selectbox("声の選択", ["女性 (七海)", "男性 (慶太)"])
        voice = "ja-JP-NanamiNeural" if "女性" in voice_role else "ja-JP-KeitaNeural"
    with col2:
        speed = st.selectbox("読み上げ速度", ["標準", "1.2倍速", "1.5倍速", "2.0倍速"])
        rate_map = {"標準": "+0%", "1.2倍速": "+20%", "1.5倍速": "+50%", "2.0倍速": "+100%"}
        rate = rate_map[speed]

    # 除外ワード設定
    remove_phrases = st.text_area(
        "削除したい定型文 (改行で複数指定)", 
        placeholder="ここに書いた文章が含まれていたら削除します。\n例：ブックマークありがとうございます\n例：誤字報告ありがとうございます",
        height=100
    )

# 実行ボタン
if st.button("音声化を開始する", type="primary"):
    if not url_input:
        st.warning("URLを入力してください")
    else:
        # 非同期処理を実行
        asyncio.run(process_novel(url_input, remove_phrases, voice, rate))
```

3. 使い方
コマンドプロンプトで以下を実行してアプリを起動します。

```bash
streamlit run app.py
```
ブラウザが立ち上がります。

「URL」欄に、読みたい小説のURL（目次でも、第1話のURLでもOK）を貼ります。

「削除したい定型文」欄に、毎回出てきて鬱陶しい言葉があれば貼り付けます（例：「評価お願いします」「次話は◯時に更新します」など）。

**「音声化を開始する」**ボタンを押します。

工夫した点（ご要望への対応）
続きから機能: フォルダ内にファイル（例: 001_xxx.mp3）があれば、その話数は何もしません。更新分だけを自動で取りに行きます。

本文オンリー: 前書き・後書きの取得処理を削除し、#novel_honbun（本文エリア）だけを狙い撃ちしています。

URL除去: 本文中に http... があっても読み飛ばします。

定型文カット: 毎回同じあとがきがある場合、その文章をコピペして設定欄に入れておけば、きれいに削除されます。

これで、ブラウザ上のメニュー画面から快適に管理できるようになります。

**User**
自分専用なのでPythonで良いよ

**Model**
仰る通り、自分一人で使うならブラウザ（Streamlit）を立ち上げるよりも、ダブルクリックしてURLを貼るだけのPythonスクリプトの方が動作も軽く、手っ取り早いですね。

ご要望の機能をすべて詰め込んだ、**「自分専用・コマンドライン版」**の完成形コードを作成しました。

このスクリプトの機能
再開機能（重要）: フォルダ内にすでにファイルがあれば秒速でスキップし、更新された最新話だけを音声化します。

本文のみ: 前書き・後書きは完全に無視します。

不要文削除: URLや、「ブックマークお願いします」などの定型文を自動削除します（設定エリアで自由に追加可能）。

URL自動補正: 1話のURLを貼っても、自動で目次ページを探して全話取得モードになります。

準備
いつものライブラリだけでOKです（tqdmは進捗バー用）。

```bash
pip install requests beautifulsoup4 edge-tts tqdm
```
決定版コード (`narou_converter.py`)
```python
import os
import re
import asyncio
import requests
from bs4 import BeautifulSoup
import edge-tts
from tqdm import tqdm

# ==========================================
# 【自分専用設定エリア】
# ここに「読み上げてほしくない定型文」をリストに追加してください
# ==========================================
IGNORE_PHRASES = [
    "ブックマーク",
    "評価をお願い",
    "感想をお待ち",
    "誤字報告",
    "次回の更新",
    "ポイントを入れて",
    "下にある☆を",
    "活動報告",
]

# 声の設定 (女性: ja-JP-NanamiNeural, 男性: ja-JP-KeitaNeural)
VOICE = "ja-JP-NanamiNeural"
RATE = "+0%"   # 速度
VOLUME = "+0%" # 音量
# ==========================================

def get_soup(url):
    """HTMLを取得"""
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        return BeautifulSoup(response.text, "html.parser")
    except Exception as e:
        print(f"Error: {e}")
        return None

def normalize_url_to_index(url):
    """1話のURLが来ても目次URLに変換する"""
    # .../nXXXXX/1/ のような末尾の数字を削る
    match = re.search(r'(https://ncode\.syosetu\.com/n[a-zA-Z0-9]+)', url)
    if match:
        return match.group(1) + "/"
    return url

def clean_text(soup):
    """本文のみ抽出し、不要な情報を削除"""
    # 本文（#novel_honbun）のみ取得（前書き・後書きは無視）
    content = soup.select_one("#novel_honbun")
    if not content:
        return ""

    # ルビ処理（親文字を消して、ふりがなを残す）
    # 例: <ruby>本気<rt>マジ</rt></ruby> → マジ
    soup_copy = BeautifulSoup(str(content), "html.parser")
    for ruby in soup_copy.find_all("ruby"):
        rt = ruby.find("rt")
        if rt:
            ruby.replace_with(rt.get_text())
    
    text = soup_copy.get_text()

    # 1. URLの削除 (http... を消す)
    text = re.sub(r'https?://[\w/:%#\$&\?\(\)~\.=\+\-]+', '', text)

    # 2. 設定した「不要な定型文」を含む行を削除
    lines = []
    for line in text.splitlines():
        line = line.strip()
        if not line:
            continue
        
        # 除外ワードが含まれていたらその行はスキップ
        is_ignore = False
        for phrase in IGNORE_PHRASES:
            if phrase in line:
                is_ignore = True
                break
        
        if not is_ignore:
            lines.append(line)

    return "\n".join(lines)

async def text_to_mp3(text, filepath):
    communicate = edge_tts.Communicate(text, VOICE, rate=RATE, volume=VOLUME)
    await communicate.save(filepath)

async def main():
    print("-" * 50)
    print(" なろう小説 音声化ツール (本文のみモード)")
    print("-" * 50)
    
    # URL入力
    input_url = input("URLを入力してください (右クリックで貼付): ").strip()
    if not input_url:
        return

    # URL補正
    target_url = normalize_url_to_index(input_url)
    
    print("小説情報を取得中...")
    soup = get_soup(target_url)
    if not soup:
        return

    # タイトル取得
    title_elem = soup.select_one(".novel_title")
    title = title_elem.get_text(strip=True) if title_elem else "不明な小説"
    safe_title = re.sub(r'[\\/:*?"<>|]', '_', title)
    
    print(f"タイトル: {title}")
    
    # 各話リスト取得
    chapters = []
    for a in soup.select(".subtitle a"):
        chap_title = a.get_text(strip=True)
        chap_url = "https://ncode.syosetu.com" + a.get("href")
        chapters.append((chap_title, chap_url))
        
    print(f"全 {len(chapters)} 話")

    # フォルダ作成
    if not os.path.exists(safe_title):
        os.makedirs(safe_title)

    # 処理開始
    new_count = 0
    skip_count = 0
    
    # tqdmで進捗バーを表示
    for i, (chap_title, chap_url) in enumerate(tqdm(chapters, desc="Progress"), 1):
        safe_chap_title = re.sub(r'[\\/:*?"<>|]', '_', chap_title)
        filename = f"{i:03}_{safe_chap_title}.mp3"
        filepath = os.path.join(safe_title, filename)

        # 【続きから機能】ファイルがあればスキップ
        if os.path.exists(filepath):
            skip_count += 1
            continue

        # 本文取得
        c_soup = get_soup(chap_url)
        if c_soup:
            body_text = clean_text(c_soup)
            
            # 音声化テキスト作成
            full_text = f"{chap_title}。\n\n{body_text}"
            
            try:
                await text_to_mp3(full_text, filepath)
                new_count += 1
            except Exception as e:
                print(f"\nError ({chap_title}): {e}")

            # サーバー負荷軽減の待機（重要）
            # 新規作成時のみ待機時間を設ける
            await asyncio.sleep(2)

    print("\n" + "=" * 50)
    print(f"完了しました！")
    print(f"・スキップ (作成済): {skip_count} 話")
    print(f"・新規作成: {new_count} 話")
    print(f"保存先: {safe_title}")
    print("=" * 50)
    
    # 完了後すぐに画面が閉じないように入力待ちにする
    input("Enterキーを押して終了してください...")

if __name__ == "__main__":
    asyncio.run(main())
```

使い方（超シンプル）
上記のコードを `narou.py` などで保存。

ファイルをダブルクリック（またはコマンドで実行）。

黒い画面が出たら、読みたい小説のURL（1話目でも目次でもOK）を貼り付けてEnter。

あとは放置。

これで、毎回「続き」から自動でダウンロードしてくれます。不要な「評価お願いします」などの文言を追加したい場合は、コード上部の `IGNORE_PHRASES` に単語を追加してください。

**User**
続きダウンロードが可能なようにするならフォルダ分け(携帯に入れるフォルダと読んだ小説の格納用)やダウンロードするのは　このフォルダ内にある小説だけ　とかも出来ると良いね。

**Model**
そのアイデア、非常に実用的ですごく良いですね！

「PCには全話を保存しておく（マスターデータ）」 「iPhoneに入れる用には、新しく更新された分だけを別フォルダに出力する」

こうすれば、iPhoneへの転送作業が「For_iPhoneフォルダの中身を全部コピーするだけ」で済み、管理が劇的に楽になります。

ご要望の機能を実装した**「完全管理型オーディオブック生成システム」**を作成しました。

新しい仕組み
フォルダ構成を自動で分けます

`Library` フォルダ：全小説・全話数が保存される（保管庫）。

`For_iPhone` フォルダ：今回新しくダウンロードした話だけが入る（転送用）。

更新チェック機能

一度ダウンロードした小説フォルダには、自動で `url.txt` というファイルを生成してURLを記憶させます。

次回からは、「一括更新モード」を選ぶだけで、フォルダにある全小説の続きを自動チェックしてダウンロードします。

決定版コード (`narou_manager.py`)
```python
import os
import re
import shutil
import asyncio
import requests
from bs4 import BeautifulSoup
import edge-tts
from tqdm import tqdm

# ==========================================
# 【設定エリア】
# ==========================================
# 1. 保存先のフォルダ名（自動作成されます）
LIBRARY_DIR = "Master_Library"  # 全データの保管場所
TRANSFER_DIR = "For_iPhone"     # iPhone転送用（新規分のみ）

# 2. 読み上げてほしくない定型文
IGNORE_PHRASES = [
    "ブックマーク", "評価", "感想", "誤字報告", "次回の更新", 
    "ポイント", "活動報告", "広告", "下記バナー"
]

# 3. 声の設定
VOICE = "ja-JP-NanamiNeural" # 女性
RATE = "+0%"
VOLUME = "+0%"
# ==========================================

def get_soup(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        return BeautifulSoup(response.text, "html.parser")
    except Exception as e:
        print(f"URLエラー: {e}")
        return None

def normalize_url(url):
    """URLを正規化（末尾の数字削除）"""
    match = re.search(r'(https://ncode\.syosetu\.com/n[a-zA-Z0-9]+)', url)
    return match.group(1) + "/" if match else url

def clean_text(soup):
    """本文抽出＆クリーニング"""
    content = soup.select_one("#novel_honbun")
    if not content: return ""

    soup_copy = BeautifulSoup(str(content), "html.parser")
    for ruby in soup_copy.find_all("ruby"):
        rt = ruby.find("rt")
        if rt: ruby.replace_with(rt.get_text())
    
    text = soup_copy.get_text()
    text = re.sub(r'https?://[\w/:%#\$&\?\(\)~\.=\+\-]+', '', text) # URL削除

    lines = []
    for line in text.splitlines():
        line = line.strip()
        if not line: continue
        if not any(phrase in line for phrase in IGNORE_PHRASES):
            lines.append(line)
    return "\n".join(lines)

async def process_novel(url, is_update_mode=False):
    """小説を処理するメイン関数"""
    url = normalize_url(url)
    soup = get_soup(url)
    if not soup: return

    # タイトル取得
    title_elem = soup.select_one(".novel_title")
    title = title_elem.get_text(strip=True) if title_elem else "不明なタイトル"
    safe_title = re.sub(r'[\\/:*?"<>|]', '_', title)

    # フォルダパス設定
    lib_path = os.path.join(LIBRARY_DIR, safe_title)
    iphone_path = os.path.join(TRANSFER_DIR, safe_title)

    if not os.path.exists(lib_path):
        os.makedirs(lib_path)
    
    # URLを記憶させる（次回更新用）
    with open(os.path.join(lib_path, "url.txt"), "w", encoding="utf-8") as f:
        f.write(url)

    # 各話リスト取得
    chapters = []
    for a in soup.select(".subtitle a"):
        chap_title = a.get_text(strip=True)
        chap_url = "https://ncode.syosetu.com" + a.get("href")
        chapters.append((chap_title, chap_url))

    print(f"確認中: {title} (全{len(chapters)}話)")

    new_files_count = 0
    
    # プログレスバー設定（更新モードのときは、明らかに新規がある場合のみ詳細表示推奨だが今回は簡易化）
    # tqdmの位置を調整して見やすく
    pbar = tqdm(chapters, desc="  Check", leave=False)
    
    for i, (chap_title, chap_url) in enumerate(pbar, 1):
        safe_chap = re.sub(r'[\\/:*?"<>|]', '_', chap_title)
        filename = f"{i:03}_{safe_chap}.mp3"
        
        file_master = os.path.join(lib_path, filename)
        
        # 【重要】マスタフォルダに既にファイルがあれば完全スキップ
        if os.path.exists(file_master):
            continue

        # ここから新規作成フロー
        pbar.set_description(f"  New: {i}話")
        
        c_soup = get_soup(chap_url)
        if c_soup:
            body = clean_text(c_soup)
            full_text = f"{chap_title}。\n\n{body}"
            
            # 1. マスターに保存
            communicate = edge_tts.Communicate(full_text, VOICE, rate=RATE, volume=VOLUME)
            await communicate.save(file_master)
            
            # 2. iPhone転送用フォルダにもコピー（フォルダが無ければ作る）
            if not os.path.exists(iphone_path):
                os.makedirs(iphone_path)
            shutil.copy2(file_master, os.path.join(iphone_path, filename))
            
            new_files_count += 1
            
            # サーバー負荷対策
            await asyncio.sleep(2)

    return title, new_files_count

async def main():
    # フォルダ準備
    if not os.path.exists(LIBRARY_DIR): os.makedirs(LIBRARY_DIR)
    if not os.path.exists(TRANSFER_DIR): os.makedirs(TRANSFER_DIR)

    print("\n=== なろう小説管理システム ===")
    print("1. 新規小説を追加・ダウンロード")
    print("2. 既存の小説を一括アップデート（続きのみ取得）")
    print("================================")
    
    mode = input("番号を選択 (1 or 2): ").strip()

    if mode == "1":
        # 新規ダウンロードモード
        url = input("URLを入力: ").strip()
        if url:
            print("\nダウンロード開始...")
            title, count = await process_novel(url)
            print(f"\n完了！ {count}話を追加しました。")
            print(f"iPhone用データ: {TRANSFER_DIR}/{title}")

    elif mode == "2":
        # 一括更新モード
        # Master_Library内のフォルダを走査して、url.txtを探す
        novels = [d for d in os.listdir(LIBRARY_DIR) if os.path.isdir(os.path.join(LIBRARY_DIR, d))]
        
        if not novels:
            print("保存されている小説がありません。まずは「1」で追加してください。")
            return

        print(f"\n保存済みフォルダから {len(novels)} 作品を確認します...\n")
        
        # 今回の転送用フォルダをクリアするか確認してもいいが、今回は追記型にする
        total_new = 0
        
        for folder_name in novels:
            folder_path = os.path.join(LIBRARY_DIR, folder_name)
            url_file = os.path.join(folder_path, "url.txt")
            
            if os.path.exists(url_file):
                with open(url_file, "r", encoding="utf-8") as f:
                    stored_url = f.read().strip()
                
                # 更新処理実行
                try:
                    title, count = await process_novel(stored_url, is_update_mode=True)
                    if count > 0:
                        print(f"  ★ 更新あり: {folder_name} (+{count}話)")
                        total_new += count
                    else:
                        # 更新なしの場合はログを出さなくても良いが、確認用に
                        pass 
                except Exception as e:
                    print(f"  エラー: {folder_name} - {e}")
            else:
                print(f"  スキップ: {folder_name} (url.txtが見つかりません)")

        print(f"\n全ての更新チェックが完了しました。")
        if total_new > 0:
            print(f"合計 {total_new} ファイルを '{TRANSFER_DIR}' に作成しました。")
            print("iPhoneに転送後、このフォルダの中身は削除して構いません。")
        else:
            print("新しい更新はありませんでした。")

    input("\nEnterキーで終了...")

if __name__ == "__main__":
    asyncio.run(main())
```
使い方の流れ
1. 初めて小説を入れる時（メニュー「1」）
起動して 1 を選びます。

URLを貼ります。

動き:

`Master_Library` フォルダに全話保存されます。

`For_iPhone` フォルダにも全話コピーされます。

フォルダ内に `url.txt` がこっそり作られ、URLが記憶されます。

2. 更新を確認したい時（メニュー「2」）
起動して 2 を選びます。

動き:

`Master_Library` にある全フォルダをチェックしに行きます。

更新がない小説は「無視（スキップ）」されます。

更新があった小説だけ、新着分をダウンロードします。

新着分だけが `For_iPhone` フォルダに出力されます。

3. iPhoneに入れる時
`For_iPhone` フォルダを開き、中にあるフォルダごと BookPlayer（クラウドストレージ）に入れます。

転送が終わったら、PC上の `For_iPhone` の中身は空にしてOKです。（マスターデータは `Master_Library` に残っているため）

これで、「読んだ小説の格納用」と「携帯に入れる用」の使い分けが完璧にできるはずです。

**User**
小説名とかはきちんと表示される？

小説家になろうだけでなく　カクヨムとかにも対応出来る？

**Model**
ご安心ください、両方ともバッチリ対応できます。

1. 小説名やタイトルの表示について
はい、きちんと表示されます。

フォルダ名: 「小説のタイトル」になります（例: 『転生したらスライムだった件』）。

ファイル名: 「001_第1話 スライム誕生.mp3」のように、連番＋各話のサブタイトルになります。

iPhoneの『BookPlayer』に入れた際、このフォルダ名が「本のタイトル」として認識され、ファイル名が「目次」としてズラッと綺麗に並びます。

2. カクヨムへの対応について
コードを少し書き換えれば対応可能です。

「なろう」と「カクヨム」は、サイトの裏側（HTMLの書き方）が違うため、これまでのコードだとエラーになります。 そこで、URLを見て「これはなろうだな」「これはカクヨムだな」と自動判別して切り替える機能を追加しました。

これで、なろうのURLを貼っても、カクヨムのURLを貼っても、同じようにダウンロード・管理できるようになります。

決定版：なろう＆カクヨム両対応・管理システム (`novel_manager_v2.py`)
以前のコードをすべて消して、以下のコードを上書きしてください。

```python
import os
import re
import shutil
import asyncio
import requests
from bs4 import BeautifulSoup
import edge-tts
from tqdm import tqdm

# ==========================================
# 【設定エリア】
# ==========================================
LIBRARY_DIR = "Master_Library"  # 保存保管用
TRANSFER_DIR = "For_iPhone"     # iPhone転送用

# 読み飛ばしたい定型文（共通）
IGNORE_PHRASES = [
    "ブックマーク", "評価", "感想", "誤字報告", "次回の更新", 
    "ポイント", "活動報告", "広告", "下記バナー", "星を", "レビュー"
]

VOICE = "ja-JP-NanamiNeural"
RATE = "+0%"
VOLUME = "+0%"
# ==========================================

def get_soup(url):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        return BeautifulSoup(response.text, "html.parser")
    except Exception as e:
        print(f"アクセスエラー: {e}")
        return None

def detect_site(url):
    """URLからサイトの種類を判定"""
    if "syosetu.com" in url:
        return "narou"
    elif "kakuyomu.jp" in url:
        return "kakuyomu"
    else:
        return "unknown"

def get_novel_info(url):
    """サイトに合わせてタイトルと各話リストを取得"""
    site_type = detect_site(url)
    soup = get_soup(url)
    if not soup: return None, [], None

    chapters = []
    title = "Unknown"

    if site_type == "narou":
        # なろうの処理
        t_elem = soup.select_one(".novel_title")
        if t_elem: title = t_elem.get_text(strip=True)
        
        for a in soup.select(".subtitle a"):
            chap_title = a.get_text(strip=True)
            chap_url = "https://ncode.syosetu.com" + a.get("href")
            chapters.append((chap_title, chap_url))

    elif site_type == "kakuyomu":
        # カクヨムの処理
        # タイトル取得 (構造が変わることがあるため複数の候補を試す)
        t_elem = soup.select_one("#workTitle") or soup.select_one("h1")
        if t_elem: title = t_elem.get_text(strip=True)

        # 各話取得
        # カクヨムは /works/XXXXX/episodes/YYYYY というリンク構造
        base_url = "https://kakuyomu.jp"
        for a in soup.select(".widget-toc-episode a"):
            chap_title = a.select_one(".widget-toc-episode-titleLabel").get_text(strip=True)
            href = a.get("href")
            if href.startswith("/"):
                chap_url = base_url + href
            else:
                chap_url = href
            chapters.append((chap_title, chap_url))
    
    else:
        print("対応していないサイトです。")
        return None, [], None

    # Windowsファイル名に使えない文字を除去
    safe_title = re.sub(r'[\\/:*?"<>|]', '_', title)
    return safe_title, chapters, site_type

def clean_text(soup, site_type):
    """サイトに合わせて本文を抽出"""
    text = ""
    
    if site_type == "narou":
        content = soup.select_one("#novel_honbun")
    elif site_type == "kakuyomu":
        content = soup.select_one(".widget-episodeBody")
    else:
        content = None

    if not content: return ""

    # ルビ処理（共通）
    soup_copy = BeautifulSoup(str(content), "html.parser")
    for ruby in soup_copy.find_all("ruby"):
        rt = ruby.find("rt")
        if rt: ruby.replace_with(rt.get_text())
    
    raw_text = soup_copy.get_text()
    
    # URL削除
    raw_text = re.sub(r'https?://[\w/:%#\$&\?\(\)~\.=\+\-]+', '', raw_text)

    # 行ごとのクリーニング
    lines = []
    for line in raw_text.splitlines():
        line = line.strip()
        if not line: continue
        # 定型文除外
        if not any(phrase in line for phrase in IGNORE_PHRASES):
            lines.append(line)
            
    return "\n".join(lines)

async def process_novel(url):
    """小説処理メイン"""
    safe_title, chapters, site_type = get_novel_info(url)
    if not safe_title: return None, 0

    # フォルダパス
    lib_path = os.path.join(LIBRARY_DIR, safe_title)
    iphone_path = os.path.join(TRANSFER_DIR, safe_title)

    if not os.path.exists(lib_path): os.makedirs(lib_path)
    
    # URL記憶
    with open(os.path.join(lib_path, "url.txt"), "w", encoding="utf-8") as f:
        f.write(url)

    print(f"確認中: {safe_title} ({site_type}) - 全{len(chapters)}話")
    
    new_count = 0
    pbar = tqdm(chapters, desc="  Check", leave=False)

    for i, (chap_title, chap_url) in enumerate(pbar, 1):
        safe_chap = re.sub(r'[\\/:*?"<>|]', '_', chap_title)
        filename = f"{i:03}_{safe_chap}.mp3"
        file_master = os.path.join(lib_path, filename)

        # すでにファイルがあればスキップ
        if os.path.exists(file_master):
            continue

        # 新規作成
        pbar.set_description(f"  New: {i}話")
        
        c_soup = get_soup(chap_url)
        if c_soup:
            body = clean_text(c_soup, site_type)
            full_text = f"{chap_title}。\n\n{body}"
            
            # 音声化
            communicate = edge_tts.Communicate(full_text, VOICE, rate=RATE, volume=VOLUME)
            await communicate.save(file_master)
            
            # iPhone用フォルダへコピー
            if not os.path.exists(iphone_path): os.makedirs(iphone_path)
            shutil.copy2(file_master, os.path.join(iphone_path, filename))
            
            new_count += 1
            await asyncio.sleep(2) # 負荷軽減

    return safe_title, new_count

async def main():
    if not os.path.exists(LIBRARY_DIR): os.makedirs(LIBRARY_DIR)
    if not os.path.exists(TRANSFER_DIR): os.makedirs(TRANSFER_DIR)

    print("\n=== なろう＆カクヨム 音声化ツール ===")
    print("1. 新規追加 (URL入力)")
    print("2. 一括更新 (全フォルダチェック)")
    
    mode = input("番号を選択 (1 or 2): ").strip()

    if mode == "1":
        url = input("URLを入力 (なろう or カクヨム): ").strip()
        # カクヨムのURL調整（/works/xxxx までにする）
        if "kakuyomu.jp" in url:
            match = re.search(r'(https://kakuyomu\.jp/works/\d+)', url)
            if match: url = match.group(1)

        if url:
            title, count = await process_novel(url)
            if title:
                print(f"\n完了: {count}話を追加 -> '{TRANSFER_DIR}/{title}'")

    elif mode == "2":
        novels = [d for d in os.listdir(LIBRARY_DIR) if os.path.isdir(os.path.join(LIBRARY_DIR, d))]
        print(f"\n{len(novels)}作品の更新をチェック中...")
        
        total_new = 0
        for folder in novels:
            url_file = os.path.join(LIBRARY_DIR, folder, "url.txt")
            if os.path.exists(url_file):
                with open(url_file, "r", encoding="utf-8") as f:
                    url = f.read().strip()
                try:
                    title, count = await process_novel(url)
                    if count > 0:
                        print(f"  ★更新: {folder} (+{count}話)")
                        total_new += count
                except Exception as e:
                    print(f"  エラー: {folder} - {e}")
        
        print(f"\n更新完了。合計 {total_new} ファイルを転送用フォルダに作成しました。")

    input("\nEnterで終了...")

if __name__ == "__main__":
    asyncio.run(main())
```

追加・変更された点
カクヨム対応:

URLの中に `kakuyomu.jp` が含まれていたら、自動的に「カクヨム用の読み取りモード」に切り替わります。

カクヨム特有のHTML構造（`widget-episodeBody`など）に対応しました。

カクヨムのURL処理:

カクヨムで1話目を開いているURLを貼っても、自動的に「作品トップページ（目次）」に変換して処理するようにしました。

除外ワードの強化:

カクヨム特有の「星を投げてください」「レビューお願いします」なども消せるよう、`IGNORE_PHRASES` に単語を追加しておきました。

これで、なろうもカクヨムも、ごちゃ混ぜで管理・更新できます。ぜひお試しください！

**User**
exe化のやり方も書いておいて

わかるけど、　機能ダウンロードとか必要かもだし

**Model**
おっしゃる通り、`edge-tts` などのライブラリを含んだ状態できちんと動く `.exe` にするには、いくつかポイントがあります（特にウイルス対策ソフトの誤検知や、コンソール画面の設定など）。

確実な手順をまとめました。

1. 準備：PyInstallerのインストール
まだ入れていなければ、コマンドプロンプトで以下を実行します。

```bash
pip install pyinstaller
```

2. exe化の実行コマンド
スクリプトがあるフォルダでコマンドプロンプトを開き、以下のコマンドを実行します。 ※ファイル名が `narou_manager_v2.py` だと仮定しています。

```bash
pyinstaller --onefile --clean --name "NarouAudio" narou_manager_v2.py
```
オプションの意味:

`--onefile`: バラバラのファイルではなく、1つのexeファイルにまとめます（扱いやすくなります）。

`--clean`: キャッシュをクリアしてビルドします（エラー防止）。

`--name "NarouAudio"`: 出来上がるファイル名を `NarouAudio.exe` に指定します。

注意: `--noconsole` や `--windowed` は付けないでください。このツールは黒い画面（コンソール）で文字入力する必要があるため、それらをつけると何も表示されずに動いてしまいます。

3. 生成物の確認
処理が終わると、フォルダ内にいくつかフォルダができます。

`dist` フォルダの中に `NarouAudio.exe` が完成しています。

これ以外の `build` フォルダや `.spec` ファイルは削除して構いません。

⚠️ exe化における注意点・トラブルシューティング
1. ウイルス対策ソフトの誤検知 PyInstallerで作った個人製exeは、Microsoft Defenderやウイルスバスターに「未知の脅威（トロイの木馬など）」として誤検知され、即座に削除されることがあります。

対策: 実行前に、そのフォルダをウイルス対策ソフトの「除外リスト」に入れるか、検知された際に「デバイスで許可」を選んで復元してください。

2. 実行場所とフォルダ生成 このexeは「exeファイルがある場所」に `Master_Library` などのフォルダを生成します。

推奨: デスクトップなどに「小説ツール」というフォルダを作り、その中にexeを入れて使うと散らかりません。

3. ファイアウォールの警告 初回起動時、`edge-tts` がMicrosoftのサーバーと通信を行うため、Windowsファイアウォールの警告が出ることがあります。

対策: 「アクセスを許可」を押してください。

4. 外部ツールのダウンロード（ffmpegなど）は不要？ 今回のコードで使用している `communicate.save` メソッドは、純粋なPythonだけで音声データを保存するため、ffmpeg.exeなどを別途ダウンロード・同梱する必要はありません。 生成された `NarouAudio.exe` 1つだけで動作します。

これで、自分専用の最強アプリとして持ち運びや配布（自分用ですが）が可能になります！

**User**
BookPlayer というアプリが複数あるが　どちらもダウンロード数100件とかだけど大丈夫？



画面が英語でよくわからない

**Model**
ストップです！そのアプリは入れないでください。 「ダウンロード数100件」や「画面が怪しい英語」のものは、本家の偽物（コピーアプリ）の可能性があります。本物はもっと評価数が多いですが、少し探しにくいのかもしれません。

また、BookPlayerは非常にシンプルで優秀ですが、メニューが英語メインなので、もし英語に抵抗がある場合は使いにくいと思います。

代わりに、日本語に完全対応していて、かつ**「PCで作ったフォルダをそのまま再生できる」**一番安全で使いやすいアプリを紹介します。

おすすめ代用アプリ：『Evermusic』（エバーミュージック）
このアプリなら、今回の「Pythonで作った小説フォルダ」を聴くのに最適です。

日本語対応: メニューが全部日本語です。

有名: 評価数が数万件ある定番アプリです。

クラウド直接再生: iCloud DriveやGoogle Driveのフォルダを直接読み込めます。

手順（Pythonで作ったデータを聴く方法）
App Storeで検索: 「Evermusic」と検索して、オレンジ色のアイコンのアプリ（無料版でOK）をダウンロードします。

クラウドと連携: アプリを開き、下のメニュー「接続」→「クラウドサービスを接続」→「iCloud Drive」（またはGoogle Drive）を選びます。

フォルダを開く: Pythonツールが作った「小説のタイトルが入ったフォルダ」が見えるはずです。

再生: そのフォルダを開いて、1話目をタップすれば再生が始まります。

倍速再生: プレーヤー画面で速度変更できます。

バックグラウンド: 画面を閉じても再生されます。

スリープタイマー: 時計アイコンから設定できます。

もし「BookPlayer」の本家を入れたい場合
どうしてもBookPlayerが良い（デザインが好きなど）場合は、以下の作者のものが「本物」です。

アプリ名: BookPlayer

作者名: Gianni Carlo

アイコン: 白黒のシンプルなデザイン（または少しカラフルな本のマーク）

特徴: 完全に無料のオープンソースアプリです。

ただ、今回は**「Evermusic」**の方が日本語で設定でき、トラブルも少ないので強くおすすめします。

先ほどのPythonツールで作ったデータは、これらのアプリですぐに聴けるようになりますよ！

